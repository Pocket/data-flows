{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting Started","text":"<p><code>data_products</code> consists of Prefect flows and shared utilities.  All flows in the project can leverage these shared utilities for the use of common abstractions specific to this project. Flows themselves live in a folder specific to that flow, which can also contain specific helpers to aid in flow development.</p> <p>Flows can be thought of as data applications that run on a specific schedule or on-demand via manual execution.</p> <p>Flows and flow specific code lives in separate folders in <code>src</code>.  For example, <code>src/sql_etl</code>.  Everything in that folder will be available to the flow at run time.</p> <p>Shared utilities live in <code>src/shared</code> can will be available to all flows.</p>"},{"location":"#installation-and-setup","title":"Installation and Setup","text":"<p>This project uses Poetry for dependency management. We also use the (Poetry dotenv plugin)[https://github.com/volopivoshenko/poetry-plugin-dotenv] to apply environment variables at runtime through the use of a <code>.env</code> file that should live at the root of this project.</p> <p>Note</p> <p>If you get a dependency error on installion of the plug, you just need to update Poetry using <code>poetry self update</code>.</p> <p>These should be deployed using the details offered by each project's respective documentation.</p> <p>Once all Poetry requirements are installed.  You can install all project dependencies through the <code>poetry install</code> command.  This will install both <code>dev</code> and <code>main</code> sets of dependencies.</p>"},{"location":"flows/sql_etl/code/","title":"Code","text":""},{"location":"flows/sql_etl/code/#src.sql_etl.run_jobs_flow","title":"<code>src.sql_etl.run_jobs_flow</code>","text":""},{"location":"flows/sql_etl/code/#src.sql_etl.run_jobs_flow-classes","title":"Classes","text":""},{"location":"flows/sql_etl/code/#src.sql_etl.run_jobs_flow.SqlEtlJob","title":"<code>SqlEtlJob</code>","text":"<p>             Bases: <code>SqlJob</code></p> <p>Model for parameters to passed to an etl job request.</p> Source code in <code>src/sql_etl/run_jobs_flow.py</code> <pre><code>class SqlEtlJob(SqlJob):\n\"\"\"Model for parameters to passed to an etl job request.\"\"\"\n\n    snowflake_stage_id: str = \"default\"\n    with_external_state: bool = False\n\n    def _init_private_attributes(self) -&gt; None:\n\"\"\"Overriding the private attributes set to include\n        os.path.join(get_script_path(), \"sql\") as a default.\n        \"\"\"\n        super()._init_private_attributes()\n        self._sql_template_path = SharedUtilsSettings().sql_template_path or os.path.join(get_script_path(), \"sql\")  # type: ignore  # noqa: E501\n\n    @property\n    def has_extraction_sql(self):\n\"\"\"Property for initiating case logic based on existence\n        of data.sql file.\n\n        Returns:\n            bool: Does file exist or not.\n        \"\"\"\n        return Path(os.path.join(self.job_file_path, \"data.sql\")).exists()\n\n    @property\n    def has_load_sql(self):\n\"\"\"Property for initiating case logic based on existence\n        of load.sql file.\n\n        Returns:\n            bool: Does file exist or not.\n        \"\"\"\n        return Path(os.path.join(self.job_file_path, \"load.sql\")).exists()\n\n    @property\n    def is_incremental(self):\n\"\"\"Logic for this property is based on existence of offset.sql\n        or with_external_state property.\n\n        Returns:\n            bool: is incremental?\n        \"\"\"\n        return (\n            Path(os.path.join(self.job_file_path, \"offset.sql\")).exists()\n            or self.with_external_state\n        )\n\n    @property\n    def default_table_name(self):\n\"\"\"Default table name based on sql folder that can be used.\n\n        Returns:\n            str: default table name\n        \"\"\"\n        return self.sql_folder_name.split(\"/\")[-1]\n\n    @property\n    def snowflake_stage(self) -&gt; SfGcsStage:\n\"\"\"Get Snowflake Gcp Stage to use based on deployment type.\n\n        Returns:\n            SfGcsStage: Model for stage metadata.\n        \"\"\"\n        return get_gcs_stage(self.snowflake_stage_id)\n\n    def get_gcs_uri(self, interval_input: IntervalSet) -&gt; str:\n\"\"\"Get the Gcp storage uri based on interval metadata.\n\n        Args:\n            interval_input (IntervalSet): Interval metadata.\n\n        Returns:\n            str: Full gcp storage uri as string.\n        \"\"\"\n        return os.path.join(\n            self.snowflake_stage.stage_location,\n            self.sql_folder_name,\n            interval_input.partition_folders,\n            \"data*.parq\",\n        )\n\n    def get_snowflake_stage_uri(self, interval_input: IntervalSet) -&gt; str:\n\"\"\"Get the Snowflake stage uri based on interval metadata.\n\n        Args:\n            interval_input (IntervalSet): Interval metadata.\n\n        Returns:\n            str: str: Full Snowflake stage uri as string.\n        \"\"\"\n        return f\"@{os.path.join(self.snowflake_stage.stage_name, self.sql_folder_name, interval_input.partition_folders)}\"  # noqa: E501\n\n    def get_last_offset_sql(self) -&gt; SqlStmt:\n\"\"\"Provide rendered current offset SQL to be executed.\n\n        Returns:\n            (SqlStmt): SqlStmt object with sql text and db engine.\n        \"\"\"\n        if self.with_external_state:\n            return self.render_sql_string(LAST_OFFSET_SQL)\n        else:\n            return self.render_sql_file(\"offset.sql\")\n\n    def get_extraction_sql(self, interval_input: IntervalSet) -&gt; SqlStmt:\n\"\"\"Provide rendered query to be passed for extraction.\n\n        Args:\n            interval_input (IntervalSet): Interval metadata.\n\n        Returns:\n            (SqlStmt): SqlStmt object with sql text and db engine.\n        \"\"\"\n        # templates for wrapping a SQL query into unload statement\n        sf_extraction_sql = \"\"\"{% set sql_engine = \"snowflake\" %}\n        copy into '{{ snowflake_stage_uri }}/data'\n        from ({{ sql }})\n        header = true\n        overwrite = true\n        max_file_size = 104857600\"\"\"  # noqa: E501\n\n        bq_extraction_sql = \"\"\"{% set sql_engine = \"bigquery\" %}\n        EXPORT DATA OPTIONS(\n          uri='{{ gcs_uri }}',\n          format='PARQUET',\n          compression='SNAPPY',\n          overwrite=true) AS\n        {{ sql }}\"\"\"  # noqa: E501\n\n        source_mapping = {\"snowflake\": sf_extraction_sql, \"bigquery\": bq_extraction_sql}\n\n        extra_kwargs = {\n            \"snowflake_stage_uri\": self.get_snowflake_stage_uri(interval_input),\n            \"gcs_uri\": self.get_gcs_uri(interval_input),\n        }\n        extra_kwargs.update(interval_input.dict())\n        sql_query = self.render_sql_file(\"data.sql\", extra_kwargs=extra_kwargs)\n        extra_kwargs[\"sql\"] = sql_query.sql_text\n        # only wrapping snowflake and bigquery in export logic\n        if x := source_mapping.get(sql_query.sql_engine):\n            extraction_sql_stmt = self.render_sql_string(x, extra_kwargs=extra_kwargs)\n        else:\n            extraction_sql_stmt = sql_query\n        return extraction_sql_stmt\n\n    def get_new_offset_sql(self, interval_input: IntervalSet) -&gt; SqlStmt:\n\"\"\"Provide rendered new offset SQL to be executed.\n\n        Args:\n            interval_input (IntervalSet): Interval metadata.\n\n        Returns:\n            (SqlStmt): SqlStmt object with sql text and db engine.\n        \"\"\"\n        extra_kwargs = {\"for_new_offset\": True}\n        extra_kwargs.update(interval_input.dict())\n        return self.render_sql_file(\"data.sql\", extra_kwargs)\n\n    def get_persist_offset_sql(self, new_offset: str) -&gt; SqlStmt:\n\"\"\"Provide rendered persist offset SQL to be executed.\n\n        Args:\n            new_offset (str): New offset to persist to external state.\n\n        Returns:\n            (SqlStmt): SqlStmt object with sql text and db engine.\n        \"\"\"\n        return self.render_sql_string(PERSIST_STATE_SQL, {\"new_offset\": new_offset})\n\n    def get_file_list_sql(self, interval_input: IntervalSet) -&gt; SqlStmt:\n\"\"\"Provide rendered list files in stage SQL to be executed.\n\n        Args:\n            interval_input (IntervalSet): Interval metadata.\n\n        Returns:\n            (SqlStmt): SqlStmt object with sql text and db engine.\n        \"\"\"\n        extra_kwargs = {\n            \"stage_name\": self.snowflake_stage,\n            \"partition_date_folder\": interval_input.partition_date_folder,\n        }\n        return self.render_sql_string(EXISTING_FILES_SQL, extra_kwargs)\n\n    def get_file_remove_sql(self, old_partition_folders: str) -&gt; SqlStmt:\n\"\"\"Provide rendered SQL to be remove files from stage.\n\n        Args:\n            old_partition_folders (str): file suffix to use for removal.\n\n        Returns:\n            (SqlStmt): SqlStmt object with sql text and db engine.\n        \"\"\"\n        extra_kwargs = {\n            \"stage_name\": self.snowflake_stage,\n            \"old_partition_folders\": old_partition_folders,\n        }\n        return self.render_sql_string(REMOVE_FILE_SQL, extra_kwargs)\n\n    def get_load_sql(self, interval_input: IntervalSet) -&gt; SqlStmt:\n\"\"\"Provide rendered SQL for post extract load.\n\n        Args:\n            interval_input (IntervalSet): Interval metadata.\n\n        Returns:\n            (SqlStmt): SqlStmt object with sql text and db engine.\n        \"\"\"\n        load_sql_file_name = \"load.sql\"\n        extra_kwargs = {\n            \"snowflake_stage_uri\": self.get_snowflake_stage_uri(interval_input),\n            \"partition_timestamp\": interval_input.partition_timestamp,\n            \"metadata_column_definitions\": \"\"\"_gs_file_name string,\n            _gs_file_row_number number,\n            _gs_file_date string,\n            _gs_file_time string,\n            _loaded_at timestamp_tz\"\"\",\n            \"metadata_keys\": \"\"\"_gs_file_name,\n            _gs_file_row_number,\n            _gs_file_date,\n            _gs_file_time,\n            _loaded_at\"\"\",\n            \"metadata_values\": \"\"\"metadata$filename,\n            metadata$file_row_number,\n            split_part(metadata$filename,'/', -3),\n            split_part(metadata$filename,'/', -2),\n            sysdate()\"\"\",\n            \"table_name\": self.default_table_name,\n        }\n        extra_kwargs.update(interval_input.dict())\n        return self.render_sql_file(load_sql_file_name, extra_kwargs)\n</code></pre>"},{"location":"flows/sql_etl/code/#src.sql_etl.run_jobs_flow.SqlEtlJob-attributes","title":"Attributes","text":""},{"location":"flows/sql_etl/code/#src.sql_etl.run_jobs_flow.SqlEtlJob.default_table_name","title":"<code>default_table_name</code>  <code>property</code>","text":"<p>Default table name based on sql folder that can be used.</p> <p>Returns:</p> Name Type Description <code>str</code> <p>default table name</p>"},{"location":"flows/sql_etl/code/#src.sql_etl.run_jobs_flow.SqlEtlJob.has_extraction_sql","title":"<code>has_extraction_sql</code>  <code>property</code>","text":"<p>Property for initiating case logic based on existence of data.sql file.</p> <p>Returns:</p> Name Type Description <code>bool</code> <p>Does file exist or not.</p>"},{"location":"flows/sql_etl/code/#src.sql_etl.run_jobs_flow.SqlEtlJob.has_load_sql","title":"<code>has_load_sql</code>  <code>property</code>","text":"<p>Property for initiating case logic based on existence of load.sql file.</p> <p>Returns:</p> Name Type Description <code>bool</code> <p>Does file exist or not.</p>"},{"location":"flows/sql_etl/code/#src.sql_etl.run_jobs_flow.SqlEtlJob.is_incremental","title":"<code>is_incremental</code>  <code>property</code>","text":"<p>Logic for this property is based on existence of offset.sql or with_external_state property.</p> <p>Returns:</p> Name Type Description <code>bool</code> <p>is incremental?</p>"},{"location":"flows/sql_etl/code/#src.sql_etl.run_jobs_flow.SqlEtlJob.snowflake_stage","title":"<code>snowflake_stage: SfGcsStage</code>  <code>property</code>","text":"<p>Get Snowflake Gcp Stage to use based on deployment type.</p> <p>Returns:</p> Name Type Description <code>SfGcsStage</code> <code>SfGcsStage</code> <p>Model for stage metadata.</p>"},{"location":"flows/sql_etl/code/#src.sql_etl.run_jobs_flow.SqlEtlJob-functions","title":"Functions","text":""},{"location":"flows/sql_etl/code/#src.sql_etl.run_jobs_flow.SqlEtlJob.get_extraction_sql","title":"<code>get_extraction_sql</code>","text":"<p>Provide rendered query to be passed for extraction.</p> <p>Parameters:</p> Name Type Description Default <code>interval_input</code> <code>IntervalSet</code> <p>Interval metadata.</p> required <p>Returns:</p> Type Description <code>SqlStmt</code> <p>SqlStmt object with sql text and db engine.</p> Source code in <code>src/sql_etl/run_jobs_flow.py</code> <pre><code>def get_extraction_sql(self, interval_input: IntervalSet) -&gt; SqlStmt:\n\"\"\"Provide rendered query to be passed for extraction.\n\n    Args:\n        interval_input (IntervalSet): Interval metadata.\n\n    Returns:\n        (SqlStmt): SqlStmt object with sql text and db engine.\n    \"\"\"\n    # templates for wrapping a SQL query into unload statement\n    sf_extraction_sql = \"\"\"{% set sql_engine = \"snowflake\" %}\n    copy into '{{ snowflake_stage_uri }}/data'\n    from ({{ sql }})\n    header = true\n    overwrite = true\n    max_file_size = 104857600\"\"\"  # noqa: E501\n\n    bq_extraction_sql = \"\"\"{% set sql_engine = \"bigquery\" %}\n    EXPORT DATA OPTIONS(\n      uri='{{ gcs_uri }}',\n      format='PARQUET',\n      compression='SNAPPY',\n      overwrite=true) AS\n    {{ sql }}\"\"\"  # noqa: E501\n\n    source_mapping = {\"snowflake\": sf_extraction_sql, \"bigquery\": bq_extraction_sql}\n\n    extra_kwargs = {\n        \"snowflake_stage_uri\": self.get_snowflake_stage_uri(interval_input),\n        \"gcs_uri\": self.get_gcs_uri(interval_input),\n    }\n    extra_kwargs.update(interval_input.dict())\n    sql_query = self.render_sql_file(\"data.sql\", extra_kwargs=extra_kwargs)\n    extra_kwargs[\"sql\"] = sql_query.sql_text\n    # only wrapping snowflake and bigquery in export logic\n    if x := source_mapping.get(sql_query.sql_engine):\n        extraction_sql_stmt = self.render_sql_string(x, extra_kwargs=extra_kwargs)\n    else:\n        extraction_sql_stmt = sql_query\n    return extraction_sql_stmt\n</code></pre>"},{"location":"flows/sql_etl/code/#src.sql_etl.run_jobs_flow.SqlEtlJob.get_file_list_sql","title":"<code>get_file_list_sql</code>","text":"<p>Provide rendered list files in stage SQL to be executed.</p> <p>Parameters:</p> Name Type Description Default <code>interval_input</code> <code>IntervalSet</code> <p>Interval metadata.</p> required <p>Returns:</p> Type Description <code>SqlStmt</code> <p>SqlStmt object with sql text and db engine.</p> Source code in <code>src/sql_etl/run_jobs_flow.py</code> <pre><code>def get_file_list_sql(self, interval_input: IntervalSet) -&gt; SqlStmt:\n\"\"\"Provide rendered list files in stage SQL to be executed.\n\n    Args:\n        interval_input (IntervalSet): Interval metadata.\n\n    Returns:\n        (SqlStmt): SqlStmt object with sql text and db engine.\n    \"\"\"\n    extra_kwargs = {\n        \"stage_name\": self.snowflake_stage,\n        \"partition_date_folder\": interval_input.partition_date_folder,\n    }\n    return self.render_sql_string(EXISTING_FILES_SQL, extra_kwargs)\n</code></pre>"},{"location":"flows/sql_etl/code/#src.sql_etl.run_jobs_flow.SqlEtlJob.get_file_remove_sql","title":"<code>get_file_remove_sql</code>","text":"<p>Provide rendered SQL to be remove files from stage.</p> <p>Parameters:</p> Name Type Description Default <code>old_partition_folders</code> <code>str</code> <p>file suffix to use for removal.</p> required <p>Returns:</p> Type Description <code>SqlStmt</code> <p>SqlStmt object with sql text and db engine.</p> Source code in <code>src/sql_etl/run_jobs_flow.py</code> <pre><code>def get_file_remove_sql(self, old_partition_folders: str) -&gt; SqlStmt:\n\"\"\"Provide rendered SQL to be remove files from stage.\n\n    Args:\n        old_partition_folders (str): file suffix to use for removal.\n\n    Returns:\n        (SqlStmt): SqlStmt object with sql text and db engine.\n    \"\"\"\n    extra_kwargs = {\n        \"stage_name\": self.snowflake_stage,\n        \"old_partition_folders\": old_partition_folders,\n    }\n    return self.render_sql_string(REMOVE_FILE_SQL, extra_kwargs)\n</code></pre>"},{"location":"flows/sql_etl/code/#src.sql_etl.run_jobs_flow.SqlEtlJob.get_gcs_uri","title":"<code>get_gcs_uri</code>","text":"<p>Get the Gcp storage uri based on interval metadata.</p> <p>Parameters:</p> Name Type Description Default <code>interval_input</code> <code>IntervalSet</code> <p>Interval metadata.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Full gcp storage uri as string.</p> Source code in <code>src/sql_etl/run_jobs_flow.py</code> <pre><code>def get_gcs_uri(self, interval_input: IntervalSet) -&gt; str:\n\"\"\"Get the Gcp storage uri based on interval metadata.\n\n    Args:\n        interval_input (IntervalSet): Interval metadata.\n\n    Returns:\n        str: Full gcp storage uri as string.\n    \"\"\"\n    return os.path.join(\n        self.snowflake_stage.stage_location,\n        self.sql_folder_name,\n        interval_input.partition_folders,\n        \"data*.parq\",\n    )\n</code></pre>"},{"location":"flows/sql_etl/code/#src.sql_etl.run_jobs_flow.SqlEtlJob.get_last_offset_sql","title":"<code>get_last_offset_sql</code>","text":"<p>Provide rendered current offset SQL to be executed.</p> <p>Returns:</p> Type Description <code>SqlStmt</code> <p>SqlStmt object with sql text and db engine.</p> Source code in <code>src/sql_etl/run_jobs_flow.py</code> <pre><code>def get_last_offset_sql(self) -&gt; SqlStmt:\n\"\"\"Provide rendered current offset SQL to be executed.\n\n    Returns:\n        (SqlStmt): SqlStmt object with sql text and db engine.\n    \"\"\"\n    if self.with_external_state:\n        return self.render_sql_string(LAST_OFFSET_SQL)\n    else:\n        return self.render_sql_file(\"offset.sql\")\n</code></pre>"},{"location":"flows/sql_etl/code/#src.sql_etl.run_jobs_flow.SqlEtlJob.get_load_sql","title":"<code>get_load_sql</code>","text":"<p>Provide rendered SQL for post extract load.</p> <p>Parameters:</p> Name Type Description Default <code>interval_input</code> <code>IntervalSet</code> <p>Interval metadata.</p> required <p>Returns:</p> Type Description <code>SqlStmt</code> <p>SqlStmt object with sql text and db engine.</p> Source code in <code>src/sql_etl/run_jobs_flow.py</code> <pre><code>def get_load_sql(self, interval_input: IntervalSet) -&gt; SqlStmt:\n\"\"\"Provide rendered SQL for post extract load.\n\n    Args:\n        interval_input (IntervalSet): Interval metadata.\n\n    Returns:\n        (SqlStmt): SqlStmt object with sql text and db engine.\n    \"\"\"\n    load_sql_file_name = \"load.sql\"\n    extra_kwargs = {\n        \"snowflake_stage_uri\": self.get_snowflake_stage_uri(interval_input),\n        \"partition_timestamp\": interval_input.partition_timestamp,\n        \"metadata_column_definitions\": \"\"\"_gs_file_name string,\n        _gs_file_row_number number,\n        _gs_file_date string,\n        _gs_file_time string,\n        _loaded_at timestamp_tz\"\"\",\n        \"metadata_keys\": \"\"\"_gs_file_name,\n        _gs_file_row_number,\n        _gs_file_date,\n        _gs_file_time,\n        _loaded_at\"\"\",\n        \"metadata_values\": \"\"\"metadata$filename,\n        metadata$file_row_number,\n        split_part(metadata$filename,'/', -3),\n        split_part(metadata$filename,'/', -2),\n        sysdate()\"\"\",\n        \"table_name\": self.default_table_name,\n    }\n    extra_kwargs.update(interval_input.dict())\n    return self.render_sql_file(load_sql_file_name, extra_kwargs)\n</code></pre>"},{"location":"flows/sql_etl/code/#src.sql_etl.run_jobs_flow.SqlEtlJob.get_new_offset_sql","title":"<code>get_new_offset_sql</code>","text":"<p>Provide rendered new offset SQL to be executed.</p> <p>Parameters:</p> Name Type Description Default <code>interval_input</code> <code>IntervalSet</code> <p>Interval metadata.</p> required <p>Returns:</p> Type Description <code>SqlStmt</code> <p>SqlStmt object with sql text and db engine.</p> Source code in <code>src/sql_etl/run_jobs_flow.py</code> <pre><code>def get_new_offset_sql(self, interval_input: IntervalSet) -&gt; SqlStmt:\n\"\"\"Provide rendered new offset SQL to be executed.\n\n    Args:\n        interval_input (IntervalSet): Interval metadata.\n\n    Returns:\n        (SqlStmt): SqlStmt object with sql text and db engine.\n    \"\"\"\n    extra_kwargs = {\"for_new_offset\": True}\n    extra_kwargs.update(interval_input.dict())\n    return self.render_sql_file(\"data.sql\", extra_kwargs)\n</code></pre>"},{"location":"flows/sql_etl/code/#src.sql_etl.run_jobs_flow.SqlEtlJob.get_persist_offset_sql","title":"<code>get_persist_offset_sql</code>","text":"<p>Provide rendered persist offset SQL to be executed.</p> <p>Parameters:</p> Name Type Description Default <code>new_offset</code> <code>str</code> <p>New offset to persist to external state.</p> required <p>Returns:</p> Type Description <code>SqlStmt</code> <p>SqlStmt object with sql text and db engine.</p> Source code in <code>src/sql_etl/run_jobs_flow.py</code> <pre><code>def get_persist_offset_sql(self, new_offset: str) -&gt; SqlStmt:\n\"\"\"Provide rendered persist offset SQL to be executed.\n\n    Args:\n        new_offset (str): New offset to persist to external state.\n\n    Returns:\n        (SqlStmt): SqlStmt object with sql text and db engine.\n    \"\"\"\n    return self.render_sql_string(PERSIST_STATE_SQL, {\"new_offset\": new_offset})\n</code></pre>"},{"location":"flows/sql_etl/code/#src.sql_etl.run_jobs_flow.SqlEtlJob.get_snowflake_stage_uri","title":"<code>get_snowflake_stage_uri</code>","text":"<p>Get the Snowflake stage uri based on interval metadata.</p> <p>Parameters:</p> Name Type Description Default <code>interval_input</code> <code>IntervalSet</code> <p>Interval metadata.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>str: Full Snowflake stage uri as string.</p> Source code in <code>src/sql_etl/run_jobs_flow.py</code> <pre><code>def get_snowflake_stage_uri(self, interval_input: IntervalSet) -&gt; str:\n\"\"\"Get the Snowflake stage uri based on interval metadata.\n\n    Args:\n        interval_input (IntervalSet): Interval metadata.\n\n    Returns:\n        str: str: Full Snowflake stage uri as string.\n    \"\"\"\n    return f\"@{os.path.join(self.snowflake_stage.stage_name, self.sql_folder_name, interval_input.partition_folders)}\"  # noqa: E501\n</code></pre>"},{"location":"flows/sql_etl/code/#src.sql_etl.run_jobs_flow-functions","title":"Functions","text":""},{"location":"flows/sql_etl/code/#src.sql_etl.run_jobs_flow.interval","title":"<code>interval</code>  <code>async</code>","text":"<p>Subflow for executing etl tasks for a single interval. Each query call will leverage run_query_task helper function to automate sending query to proper engine.</p> <p>Parameters:</p> Name Type Description Default <code>etl_input</code> <code>SqlEtlJob</code> <p>Sql job input parameters.</p> required <code>interval_input</code> <code>IntervalSet</code> <p>Interval set metadata.</p> required Source code in <code>src/sql_etl/run_jobs_flow.py</code> <pre><code>@flow(description=\"Interval flow for query based extractions from Snowflake.\")\nasync def interval(etl_input: SqlEtlJob, interval_input: IntervalSet):\n\"\"\"Subflow for executing etl tasks for a single interval.\n    Each query call will leverage run_query_task helper function\n    to automate sending query to proper engine.\n\n    Args:\n        etl_input (SqlEtlJob): Sql job input parameters.\n        interval_input (IntervalSet): Interval set metadata.\n    \"\"\"\n    # get standard Prefect logger for logging\n    logger = get_run_logger()\n    # preventing unbound error for new_offset usage downstream\n    new_offset = [(\"1970-01-01\",)]\n    # if is incremental, track new offset\n    # get the new offset using input model property\n    if etl_input.is_incremental:\n        offset_stmt = etl_input.get_new_offset_sql(interval_input)\n        new_offset = await offset_stmt.run_query_task(\"get-new-offset\")\n        logger.info(f\"New offset will be: {new_offset[0][0]}...\")\n        # if new offset is None, that means no rows for this interval\n        if new_offset[0][0] is None or new_offset[0][0] == \"None\":\n            message = \"No rows to process...\"\n            logger.info(message)\n            return message\n        # based on the starting offset, find list of object paths to delete when...\n        # doing backfill\n        existing_files_stmt = etl_input.get_file_list_sql(interval_input)\n        existing_files = await existing_files_stmt.run_query_task(\"get-existing-files\")\n        # take the LIST statement results and provide clean deduplicated list\n        clean_up_list = get_files_for_cleanup(existing_files, interval_input)\n        # remove all the object paths identified\n\n        async def remove_file(file_path: str):\n\"\"\"internal function to wrap remove logic to be\n            used for collection of async calls.\n\n            Args:\n                file_path (str): Snowflake stage path string.\n            \"\"\"\n            sql_stmt = etl_input.get_file_remove_sql(file_path)\n            await sql_stmt.run_query_task(\"clean-up-files\")\n\n        remove_files = [await remove_file(i) for i in clean_up_list]\n    else:\n        logger.info(\"Non-incremental run...\")\n        logger.info(\"No offsets...\")\n        # need a remove_file for downstream continuation\n        remove_files = []\n    # Run extraction\n    logger.info(\"Running extraction...\")\n    extract_stmt = etl_input.get_extraction_sql(interval_input)\n    extract = await extract_stmt.run_query_task(\n        \"run-extraction\", **{\"wait_for\": [remove_files]}\n    )\n    # run a post extraction load sql if it exists\n    if etl_input.has_load_sql:\n        logger.info(\"Applying new offset...\")\n        # commit the new offset\n        load_sql = etl_input.get_load_sql(interval_input)\n        load = await load_sql.run_query_task(\"run-load\", **{\"wait_for\": [extract]})\n        logger.info(f\"Load logic completed with: {load[0]}\")\n    else:\n        load = \"No load sql to execute...\"\n        logger.info(load)\n    # persist the new offset to external snowflake state table if enabled\n    if etl_input.is_incremental:\n        if etl_input.with_external_state:\n            logger.info(\"Applying new offset...\")\n            # commit the new offset\n            persist_offset_stmt = etl_input.get_persist_offset_sql(new_offset[0][0])\n            persist_offset = await persist_offset_stmt.run_query_task(\n                \"persist-new-offset\", **{\"wait_for\": [load]}\n            )\n            logger.info(f\"Persist offset logic completed with: {persist_offset[0]}\")\n        else:\n            persist_offset = \"External state disabled. No offset to persist...\"\n            logger.info(persist_offset)\n</code></pre>"},{"location":"flows/sql_etl/code/#src.sql_etl.run_jobs_flow.main","title":"<code>main</code>  <code>async</code>","text":"<p>Main flow for iterating through etl intervals.</p> <p>Parameters:</p> Name Type Description Default <code>etl_input</code> <code>SqlEtlJob</code> <p>Sql job input parameters.</p> required Source code in <code>src/sql_etl/run_jobs_flow.py</code> <pre><code>@flow(description=\"Interval flow for query based extractions from Snowflake.\")\nasync def main(etl_input: SqlEtlJob):\n\"\"\"Main flow for iterating through etl intervals.\n\n    Args:\n        etl_input (SqlEtlJob): Sql job input parameters.\n    \"\"\"\n    # get standard Prefect logger for logging\n    logger = get_run_logger()\n\n    # helper functions to reduce code\n    async def process_intervals(etl_input):\n        if etl_input.has_extraction_sql:\n            if etl_input.is_incremental:\n                logger.info(\n                    f\"Running directory: {etl_input.sql_folder_name}...\"  # noqa: E501\n                )\n                # get the last offset\n                last_offset_stmt = etl_input.get_last_offset_sql()\n                last_offset = await last_offset_stmt.run_query_task(\"get-last-offset\")\n                logger.info(f\"Last offset is: {last_offset[0][0]}...\")\n                for i in etl_input.get_intervals(last_offset[0][0]):\n                    await interval(etl_input, i)\n            else:\n                static_datetime_str = (\n                    etl_input.override_last_offset\n                    or pdm.now(tz=\"UTC\").to_iso8601_string()\n                )\n                static_interval = IntervalSet(\n                    batch_start=static_datetime_str\n                )  # type: ignore\n                await interval(etl_input, static_interval)\n\n        else:\n            logger.info(f\"No extraction for directory: {etl_input.sql_folder_name}...\")\n\n    async def process_all():\n        sub_paths = [\n            path.parts[-1]\n            for path in Path(etl_input.job_file_path).iterdir()\n            if path.is_dir()\n        ]\n        if sub_paths:\n            logger.info(\"Sub directories exist...\")\n            task_group = []\n            for s in sub_paths:\n                new_input = deepcopy(etl_input)\n                new_folder_name = os.path.join(etl_input.sql_folder_name, s)\n                new_input.sql_folder_name = new_folder_name\n                logger.info(f\"Running for sub directory: {new_folder_name}...\")\n                task_group.append(process_intervals(new_input))\n            await process_parallel_subflows(task_group)\n        await process_intervals(etl_input)\n\n    await process_all()\n</code></pre>"},{"location":"flows/sql_etl/usage/","title":"Usage","text":"<p><code>sql_etl</code> is a Prefect flow meant to act as a service for executing simple SQL based extract and load workflows.  This means a SQL statement extracts the data to cloud storage.  Then a SQL statement loads this data into a cloud-based Data Warehouse.  That is the expectation for how this should be used.</p> <p>This is technically more of an <code>el</code>, but <code>etl</code> is a more common term to use.</p> <p>A <code>sql_etl</code> job can be incremental and use offset logic or simply just run an extract-load process without worring about tracking offset or batching by day.</p> <p>Code lives at <code>src/sql_etl</code> and is described here.</p> <p>Here is a flow diagram showing how it all works:</p> <p></p>"},{"location":"flows/sql_etl/usage/#expectations","title":"Expectations","text":"<p>To leverage this service for jobs, <code>sql_etl</code> you will need to know:</p> <ul> <li>How to create a deployment</li> <li>How to create the SQL expected for things to work</li> <li>How the offset and interval logic works</li> <li>How to setup your local environment</li> </ul> <p>Below we go through each element.</p>"},{"location":"flows/sql_etl/usage/#deployment","title":"Deployment","text":"<p>The</p>"},{"location":"flows/sql_etl/usage/#sql-statments","title":"SQL statments","text":"<p>SQL statments will live in subfolders at <code>src/sql_etl/sql</code></p> <p>Here is an example of what a set of sql job files looks like:</p> <pre><code>sql_etl\n\u2514\u2500\u2500 sql\n    \u251c\u2500\u2500 backend_events_for_mozilla\n    \u2502   \u2514\u2500\u2500 data.sql\n    \u251c\u2500\u2500 curated_feed_exports_aurora\n    \u2502   \u251c\u2500\u2500 curated_feed_items\n    \u2502   \u2502   \u251c\u2500\u2500 data.sql\n    \u2502   \u2502   \u2514\u2500\u2500 load.sql\n    \u2502   \u251c\u2500\u2500 curated_feed_items_deleted\n    \u2502   \u2502   \u251c\u2500\u2500 data.sql\n    \u2502   \u2502   \u2514\u2500\u2500 load.sql\n    \u2502   \u251c\u2500\u2500 curated_feed_prospects\n    \u2502   \u2502   \u251c\u2500\u2500 data.sql\n    \u2502   \u2502   \u2514\u2500\u2500 load.sql\n    \u2502   \u251c\u2500\u2500 curated_feed_queued_items\n    \u2502   \u2502   \u251c\u2500\u2500 data.sql\n    \u2502   \u2502   \u2514\u2500\u2500 load.sql\n    \u2502   \u2514\u2500\u2500 tile_source\n    \u2502       \u251c\u2500\u2500 data.sql\n    \u2502       \u2514\u2500\u2500 load.sql\n    \u251c\u2500\u2500 firefox_new_tab_impressions\n    \u2502   \u251c\u2500\u2500 firefox_new_tab_daily_disable_rate_by_feed\n    \u2502   \u2502   \u251c\u2500\u2500 data.sql\n    \u2502   \u2502   \u2514\u2500\u2500 offset.sql\n    \u2502   \u251c\u2500\u2500 firefox_new_tab_daily_spoc_fill_rate_by_position_feed\n    \u2502   \u2502   \u251c\u2500\u2500 data.sql\n    \u2502   \u2502   \u2514\u2500\u2500 offset.sql\n    \u2502   \u251c\u2500\u2500 firefox_new_tab_daily_unique_engagement_by_feed\n    \u2502   \u2502   \u251c\u2500\u2500 data.sql\n    \u2502   \u2502   \u2514\u2500\u2500 offset.sql\n    \u2502   \u2514\u2500\u2500 firefox_new_tab_monthly_unique_engagement_by_feed\n    \u2502       \u251c\u2500\u2500 data.sql\n    \u2502       \u2514\u2500\u2500 offset.sql\n    \u2514\u2500\u2500 impression_stats_v1\n        \u251c\u2500\u2500 data.sql\n        \u251c\u2500\u2500 load.sql\n        \u2514\u2500\u2500 offset.sql\n</code></pre> <p>Notice that we can have up to three files possible in a folder:</p> <ul> <li><code>data.sql</code></li> <li><code>load.sql</code></li> <li><code>offset.sql</code></li> </ul> <p>Also, notice that we support files living at the top level of a folder within the <code>sql</code> folder.  We also support a single level of nesting files in subdirectories of a folder in <code>sql</code>.  The <code>curated_feed_exports_aurora</code> is an example of the supporting nesting of subdirectories.  If, needed SQL files can live at the top of the parent directory and will executed after the subdirectories are completed.</p>"},{"location":"flows/sql_etl/usage/#extraction","title":"Extraction","text":"<p>At the very least your folder of SQL statements must contain a <code>data.sql</code></p> <p>This is the query which defines the data to be extracted.  </p> <p>For example:</p> <pre><code>SELECT *\nFROM TABLE_TO_BE_EXTRACTED;\n</code></pre> <p>By itself this will not do much and will fail. This is because we have not defined the <code>sql_engine</code> this needs to run on.  The engines this service supports are <code>[\"snowflake\", \"bigquery\", \"mysql\", \"postgres\"]</code>.  All SQL is run through jinja2, which means we can add templating logic to our statements.  For <code>sql_engine</code> we need a block telling us where to run this.</p> <p>For example:</p> <pre><code>{% set sql_engine = \"bigquery\" %}\nSELECT *\nFROM TABLE_TO_BE_EXTRACTED;\n</code></pre> <p>Now when the service renders and uses this extract statment, it knows where to run it.  All configuration needed for connecting the engines we support is supplied via environment variables, which will be discussed in another section.</p> <p>Note</p> <p>All SQL running through this service must have a <code>sql_engine</code> block.</p> <p>Note</p> <p>For <code>snowflake</code> and <code>bigquery</code>, we will wrap the rendered <code>data.sql</code> in specific export logic located in this method:</p> <p><code>get_extraction_sql</code></p> <p><code>mysql</code> and <code>postgres</code> will not be wrapped in any logic as of now, which means the export logic must be built into the query.</p> <p>A single <code>data.sql</code> file means you are just loading to cloud storage.</p> <p>Since these files are being formatted by jijnja2, we can add our own custom keywords to do things like this:</p> <pre><code>    SELECT\n    *   \nFROM\n{% if for_backfill %}\n    deduped_table\n{% else %}\n    live_table \n{% endif %}\nWHERE submission_timestamp &gt;= {{ helpers.parse_iso8601(batch_start) }}\nAND submission_timestamp &lt; {{ helpers.parse_iso8601(batch_end) }}\nQUALIFY row_number() over (PARTITION BY DATE(submission_timestamp),\ndocument_id\nORDER BY\nsubmission_timestamp desc) = 1\n</code></pre> <p>Also, since these are jinja2 templates we can import helpers using something like this toward the top of the file:</p> <p><code>{% import 'helpers.j2' as helpers with context %}</code></p> <p>This is what makes the <code>helpers.parse_iso8601</code> call possible in the last SQL example above.</p> <p>These parameters are available to all <code>data.sql</code> files:</p> <ul> <li>The fields of the base <code>SQLJob</code> pydantic model described here.  Your custom parameter values are defined as a dictionary in <code>kwargs</code>.</li> <li>The fields of the <code>SqlEtlJob</code> pydantic model described here</li> </ul>"},{"location":"flows/sql_etl/usage/#loading","title":"Loading","text":"<p>Loading requires the existence of a <code>load.sql</code>.</p> <p>This is the query which defines the data to be extracted.  </p> <p>For example:</p> <pre><code>{% set sql_engine = \"snowflake\" %}\nCOPY INTO TABLE_TO_BE_LOADED\nFROM @my_ext_stage\nFILE_FORMAT = (TYPE = 'PARQUET')\n</code></pre> <p>Note</p> <p>Currently all data exports from <code>snowflake</code> and <code>bigquery</code> will be in <code>PARQUET</code> format.</p>"},{"location":"flows/sql_etl/usage/#offset","title":"Offset","text":"<p>This service provides the option to run either <code>incremental</code> or <code>non-incremental</code> extract-load jobs.</p> <p>When you have the need to produce a <code>last_offset</code> to make the flow run incremental, you will need to provide an <code>offset.sql</code> file.  This activates the incremental logic.</p> <p>For example:</p> <pre><code>{% set sql_engine = \"snowflake\" %}\nselect max(timestamp_field) as last_offset\nfrom TABLE_TO_BE_LOADED\n</code></pre> <p>Pretty much anything can be in the <code>offset.sql</code>.  For example, if you want to pin the offset to the last 24 hours you could do something like this:</p> <pre><code>{% set sql_engine = \"snowflake\" %}\nselect (trunc(sysdate()::timestamp, 'day') - interval '1 day') - interval '1 microsecond';\n</code></pre> <p>If this is run on <code>2023-09-17 13:00</code>, this would result in a value of <code>2023-09-15 23:59:59.999999</code>.  When the incremental logic built into the service add 1 microsecond, the <code>data.sql</code> file will have access to a batch_start timestamp of <code>2023-09-16 00:00:00.000000</code> for a proper <code>&gt;=</code> where clause declaration.</p> <p>There is also the option to track offset using an external table called <code>sql_offset_state</code> with the <code>with_external_state</code> flag.  If this flag is set, then <code>offset.sql</code> is not required to activate incremental logic because that value is pulled from the state table.  </p> <p>Note</p> <p>Using the <code>with_external_state</code> does require the use of the <code>initial_last_offset</code> job parameter.</p> <p><code>with_external_state</code>, incremental logic, and job parameters will be discussed later on.  </p>"},{"location":"flows/sql_etl/usage/#intervals","title":"Intervals","text":"<p>The interval logic in this </p>"},{"location":"shared/utils/code/","title":"Code","text":""},{"location":"shared/utils/code/#src.shared.utils","title":"<code>src.shared.utils</code>","text":""},{"location":"shared/utils/code/#src.shared.utils-classes","title":"Classes","text":""},{"location":"shared/utils/code/#src.shared.utils.IntervalSet","title":"<code>IntervalSet</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Model to leverage for interacting with batch intervals. Optional types are to support passing an interval set for full refresh.</p> Source code in <code>src/shared/utils.py</code> <pre><code>class IntervalSet(BaseModel):\n\"\"\"Model to leverage for interacting with batch intervals.\n    Optional types are to support passing an interval set for full refresh.\n    \"\"\"\n\n    batch_start: str = Field(description=\"Interval start datetime.\")\n    batch_end: Optional[str] = Field(description=\"Interval end datetime.\")\n    first_interval_start: Optional[str] = Field(\n        description=\"First full day in interval list.\"\n    )\n    sets_end: Optional[str] = Field(\n        description=\"Batch end used to stop at.\",\n    )\n    is_initial: Optional[bool] = Field(\n        description=(\n            \"Flag to use for SQL template because the initial \"\n            \"start filter will '&gt;' as opposed to '&gt;='.\"\n        )\n    )\n    is_final: Optional[bool] = Field(\n        description=(\n            \"Flag to use for SQL template because the final interval \"\n            \"may trigger special logic for offset.\"\n        )\n    )\n\n    @property\n    def time_format_string(self) -&gt; str:\n\"\"\"Helper for getting the time format string used.\n\n        Returns:\n            str: time format string used for folder.\n        \"\"\"\n        return \"HH-mm-ss-SSS\"\n\n    @property\n    def partition_datetime(self) -&gt; DateTime:\n\"\"\"Helper for getting the DateTime object used for defining partition\n\n        Returns:\n            DateTime: DateTime object used for defining partition.\n        \"\"\"\n        return parse(self.batch_start)  # type: ignore\n\n    @property\n    def partition_date_folder(self) -&gt; str:\n\"\"\"Helper for getting the partition date folder used.\n\n        Returns:\n            str: Partition date folder used.\n        \"\"\"\n        date_str = self.partition_datetime.to_date_string()  # type: ignore\n        return f\"date={date_str}\"\n\n    @property\n    def partition_time_folder(self) -&gt; str:\n\"\"\"Helper for getting the partition time folder used.\n\n        Returns:\n            str: Partition time folder used.\n        \"\"\"\n        time_str = self.partition_datetime.format(self.time_format_string)  # type: ignore  # noqa: E501\n        return f\"time={time_str}\"\n\n    @property\n    def partition_folders(self) -&gt; str:\n\"\"\"Helper for getting date time partition folders.\n\n        Returns:\n            str: Date time partition folders.\n        \"\"\"\n        return f\"{self.partition_date_folder}/{self.partition_time_folder}\"\n\n    @property\n    def partition_timestamp(self) -&gt; int:\n\"\"\"Helper for getting unix timestamp as an int.\n\n        Returns:\n            int: Partition unix timestamp int.\n        \"\"\"\n        return self.partition_datetime.int_timestamp  # type: ignore\n</code></pre>"},{"location":"shared/utils/code/#src.shared.utils.IntervalSet-attributes","title":"Attributes","text":""},{"location":"shared/utils/code/#src.shared.utils.IntervalSet.partition_date_folder","title":"<code>partition_date_folder: str</code>  <code>property</code>","text":"<p>Helper for getting the partition date folder used.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Partition date folder used.</p>"},{"location":"shared/utils/code/#src.shared.utils.IntervalSet.partition_datetime","title":"<code>partition_datetime: DateTime</code>  <code>property</code>","text":"<p>Helper for getting the DateTime object used for defining partition</p> <p>Returns:</p> Name Type Description <code>DateTime</code> <code>DateTime</code> <p>DateTime object used for defining partition.</p>"},{"location":"shared/utils/code/#src.shared.utils.IntervalSet.partition_folders","title":"<code>partition_folders: str</code>  <code>property</code>","text":"<p>Helper for getting date time partition folders.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Date time partition folders.</p>"},{"location":"shared/utils/code/#src.shared.utils.IntervalSet.partition_time_folder","title":"<code>partition_time_folder: str</code>  <code>property</code>","text":"<p>Helper for getting the partition time folder used.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Partition time folder used.</p>"},{"location":"shared/utils/code/#src.shared.utils.IntervalSet.partition_timestamp","title":"<code>partition_timestamp: int</code>  <code>property</code>","text":"<p>Helper for getting unix timestamp as an int.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Partition unix timestamp int.</p>"},{"location":"shared/utils/code/#src.shared.utils.IntervalSet.time_format_string","title":"<code>time_format_string: str</code>  <code>property</code>","text":"<p>Helper for getting the time format string used.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>time format string used for folder.</p>"},{"location":"shared/utils/code/#src.shared.utils.SharedUtilsSettings","title":"<code>SharedUtilsSettings</code>","text":"<p>             Bases: <code>Settings</code></p> <p>Setting model to define reusable settings.</p> Source code in <code>src/shared/utils.py</code> <pre><code>class SharedUtilsSettings(Settings):\n\"\"\"Setting model to define reusable settings.\"\"\"\n\n    sql_template_path: Optional[str]\n</code></pre>"},{"location":"shared/utils/code/#src.shared.utils.SqlJob","title":"<code>SqlJob</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Model for parameters to passed to an sql job request.</p> Source code in <code>src/shared/utils.py</code> <pre><code>class SqlJob(BaseModel):\n\"\"\"Model for parameters to passed to an sql job request.\"\"\"\n\n    sql_folder_name: str = Field(\n        description=\"Relative folder name containing sql.\",\n    )\n    # setting sql_templat_path as private attribute to allow for...\n    # easier override in child classes via _init_private_attributes\n    _sql_template_path: Optional[str] = PrivateAttr()\n    initial_last_offset: Optional[str] = Field(\n        description=\"Optional initial batch start offset.\",\n    )\n    override_last_offset: Optional[str] = Field(\n        description=\"Optional batch start override to backfill as needed.\",\n    )\n    override_batch_end: Optional[str] = Field(\n        description=\"Optional batch end override to hard stop at.\",\n    )\n    include_now: bool = Field(\n        False,\n        description=(\"Whether to include up to current utc datetime in extraction\"),\n    )\n    connection_overrides: dict[str, dict] = Field(\n        {},\n        description=(\n\"\"\"Any additional keyword arguments as a dictionary\n            to pass into the connector. Must in the form of:\n\n                {\"connector_param_name\": {\n                        \"arg_name\": \"arg_value\"\n                    } \n                }\n            \"\"\"\n        ),\n    )\n    kwargs: dict = Field(\n        {},\n        description=(\n            \"Any additional keyword arguments as a dictionary\"\n            \"to pass into your templates.\"\n        ),\n    )\n\n    def _init_private_attributes(self) -&gt; None:\n\"\"\"Great way to update private attributes without\n        losing autocomplete on the model.\n\n        https://github.com/pydantic/pydantic/discussions/3512#discussioncomment-3226167\n\n        \"\"\"\n        super()._init_private_attributes()\n        self._sql_template_path = SharedUtilsSettings().sql_template_path  # type: ignore  # noqa: E501\n\n    @property\n    def job_file_path(self):\n        return os.path.join(self._sql_template_path, self.sql_folder_name)  # type: ignore  # noqa: E501\n\n    @property\n    def extras_file_path(self):\n        return os.path.join(self._sql_template_path, \"extras\")  # type: ignore  # noqa: E501\n\n    @property\n    def job_kwargs(self) -&gt; dict:\n\"\"\"Returns a flat dictionary of the kwargs parameter\n        plus the other top level parameters.  This gets passed to\n        the tempate rendering methods.\n\n        Returns:\n            dict: Model parameters as a single dict with kwargs items inline.\n        \"\"\"\n        top_level_kwargs = deepcopy(self.dict())\n        top_level_kwargs.pop(\"kwargs\")\n        job_kwargs = deepcopy(self.kwargs)\n        job_kwargs.update(top_level_kwargs)\n        return job_kwargs\n\n    def render_from_template(self, template: Template, render_kwargs: dict) -&gt; SqlStmt:\n\"\"\"Helper function\n\n        Args:\n            template (Template): jinja2 Template object to render.\n            render_kwargs (dict): kwargs to pass to render function.\n\n        Raises:\n            Exception: Exception that enforces existence of sql engine variable.\n\n        Returns:\n            SqlStmt: SqlStmt object with sql text and db engine.\n        \"\"\"\n\n        sql_engine = \"snowflake\"\n        is_multi_statement = False\n        try:\n            sql_engine = template.module.sql_engine  # type: ignore\n        except AttributeError:\n            raise Exception(\n                'SQL file must have jinja2 block {% set sql_engine = \"&lt;engine_type&gt;\" %}'\n                f\", and must be one of {QUERY_ENGINE_TYPES_SET}\"\n            )\n        try:\n            is_multi_statement = template.module.is_multi_statement  # type: ignore\n        except AttributeError:\n            pass\n        sql_text = template.render(**render_kwargs)\n        return SqlStmt(\n            sql_engine=sql_engine,\n            sql_text=sql_text,\n            is_multi_statement=is_multi_statement,\n            connection_overrides=self.connection_overrides,\n        )\n\n    def render_sql_string(self, sql_string: str, extra_kwargs: dict = {}) -&gt; SqlStmt:\n\"\"\"Helper method for rendering a jinj2 sql string\n        using job kwargs plus optional additional kwargs.\n\n        Args:\n            sql_string (str): SQL text with jijna2 template logic.\n            extra_kwargs (dict): Optional kwargs to pass into template on top of\n            the job_kwargs.\n\n        Returns:\n            SqlStmt: SqlStmt object with sql text and db engine.\n        \"\"\"\n        render_kwargs = deepcopy(self.job_kwargs)\n        render_kwargs.update(extra_kwargs)\n        environment = Environment()\n        j2_env = environment\n        template = j2_env.from_string(sql_string)\n        return self.render_from_template(template, render_kwargs)\n\n    def render_sql_file(self, sql_file: str, extra_kwargs: dict = {}) -&gt; SqlStmt:\n\"\"\"Helper method for rendering a jinja2 sql file using\n        job kwargs plus optional additional kwargs.\n\n        Args:\n            sql_file (str): File name containing SQL text with jijna2 template logic.\n            extra_kwargs (dict): Optional kwargs to pass into template on top of\n            the job_kwargs.\n\n        Returns:\n            SqlStmt: SqlStmt object with sql text and db engine.\n        \"\"\"\n        render_kwargs = deepcopy(self.job_kwargs)\n        render_kwargs.update(extra_kwargs)\n        environment = Environment(\n            loader=FileSystemLoader([self.job_file_path, self.extras_file_path]),\n        )\n        j2_env = environment\n        template = j2_env.get_template(sql_file)\n        return self.render_from_template(template, render_kwargs)\n\n    def get_intervals(self, last_offset: Union[str, None] = None) -&gt; list[IntervalSet]:\n\"\"\"Method that returns the intervals to be used for sql job.\n\n        This means that the range of time to process will be split into\n        many intervals (currently only support days) given that the range spans\n        multiple intervals.\n\n        If the range does not span multiple intervals, then the result is one\n        interval, depending on the value of the include_now parameter.\n\n        Unless include_now is True, the last interval in the list\n        will be the last full interval in the range.  For example, since we only\n        support days, if the last offset is today, there should be no intervals\n        to process, becuase the default is up through yesterday (UTC). If include_now\n        is True, then that final interval in the list will end before now (UTC) or the\n        end_date_override.\n        The existence of end_date_override ignores include_now = False.\n\n        Args:\n            last_offset (str): Last offset to use for getting proper intervals.\n            If None, then defaults to value of override or initial offsets.\n\n        Returns:\n            list[IntervalSet]: List of intervals to use for job processing.\n        \"\"\"\n        # use override end date if provided\n        # default to now UTC\n        batch_end = self.override_batch_end or pdm.now(tz=\"UTC\").to_iso8601_string()\n        # if last_offset is None, we use the initial offset\n        if last_offset is None or last_offset == \"None\":\n            last_offset = self.initial_last_offset\n        # override last offset if provided\n        last_offset_str = self.override_last_offset or str(last_offset)\n        # the resulting last_offset_str cannot be None\n        if last_offset_str is None or last_offset_str == \"None\":\n            raise ValueError(\n                \"The resulting last offset cannot be None. \"\n                \"If last_offset is None, then initial_last_offset or \"\n                \"override_last_offset must be set\"\n            )\n        # offset will be incremented by 1 microseconds\n        # to support using '&gt;=' and '&lt;' for all intervals\n        # this is less aggressive than using 1000 (1 millisecond)\n        last_offset_final = parse(last_offset_str).add(microseconds=1)  # type: ignore\n        # the calculated intervals should start from\n        # the first full interval after the last offset\n        start = last_offset_final.end_of(\"day\").add(microseconds=1)  # type: ignore\n        end = parse(batch_end)\n        # create a pendulum period\n        period_range = pdm.period(start, end, absolute=True)  # type: ignore\n        # create base list of datetime object from period\n        # only include if less then of equal to start of period\n        # end datetime\n        intervals = [\n            x\n            for x in period_range.range(\"days\")\n            if x &lt;= end.start_of(\"day\")  # type: ignore\n        ]\n        if intervals:\n            # if we have intervals, insert offset as the first\n            intervals.insert(0, last_offset_final)\n        # setting base end to the last item in list\n        # this is for include_now and override logic\n        base_end = intervals[-1]\n        # only append base_end if needed and &gt; then base_end\n        if self.include_now or self.override_batch_end:\n            if end &gt; base_end:  # type: ignore\n                intervals.append(end)\n                base_end = end\n        # create base parameters for the for loop\n        interval_len = len(intervals)\n        last_idx = interval_len - 2\n        # create the staging list of intervat sets\n        interval_sets = []\n        # set proper first_interval_start\n        first_interval_start = start\n        first_item_time = intervals[0].to_time_string()  # type: ignore\n        # this evaluation is needed because if the offset value...\n        # turns out to be the start of the day, then that is really...\n        # the first interval start and helper with more efficient file clean up\n        if first_item_time == \"00:00:00\":\n            first_interval_start = intervals[0]\n        for idx, i in enumerate(intervals):\n            is_initial = False\n            is_final = False\n            if idx == 0:\n                is_initial = True\n            if idx == last_idx:\n                is_final = True\n            end_idx = idx + 1\n            # last item in list should not be used as start\n            if end_idx == interval_len:\n                break\n            interval_sets.append(\n                IntervalSet(\n                    batch_start=i.to_iso8601_string(),  # type: ignore\n                    batch_end=intervals[end_idx].to_iso8601_string(),  # type: ignore\n                    first_interval_start=first_interval_start.to_iso8601_string(),  # type: ignore  # noqa: E501\n                    sets_end=base_end.to_iso8601_string(),  # type: ignore\n                    is_initial=is_initial,\n                    is_final=is_final,\n                )\n            )\n\n        return interval_sets\n</code></pre>"},{"location":"shared/utils/code/#src.shared.utils.SqlJob-attributes","title":"Attributes","text":""},{"location":"shared/utils/code/#src.shared.utils.SqlJob.job_kwargs","title":"<code>job_kwargs: dict</code>  <code>property</code>","text":"<p>Returns a flat dictionary of the kwargs parameter plus the other top level parameters.  This gets passed to the tempate rendering methods.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Model parameters as a single dict with kwargs items inline.</p>"},{"location":"shared/utils/code/#src.shared.utils.SqlJob-functions","title":"Functions","text":""},{"location":"shared/utils/code/#src.shared.utils.SqlJob.get_intervals","title":"<code>get_intervals</code>","text":"<p>Method that returns the intervals to be used for sql job.</p> <p>This means that the range of time to process will be split into many intervals (currently only support days) given that the range spans multiple intervals.</p> <p>If the range does not span multiple intervals, then the result is one interval, depending on the value of the include_now parameter.</p> <p>Unless include_now is True, the last interval in the list will be the last full interval in the range.  For example, since we only support days, if the last offset is today, there should be no intervals to process, becuase the default is up through yesterday (UTC). If include_now is True, then that final interval in the list will end before now (UTC) or the end_date_override. The existence of end_date_override ignores include_now = False.</p> <p>Parameters:</p> Name Type Description Default <code>last_offset</code> <code>str</code> <p>Last offset to use for getting proper intervals.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[IntervalSet]</code> <p>list[IntervalSet]: List of intervals to use for job processing.</p> Source code in <code>src/shared/utils.py</code> <pre><code>def get_intervals(self, last_offset: Union[str, None] = None) -&gt; list[IntervalSet]:\n\"\"\"Method that returns the intervals to be used for sql job.\n\n    This means that the range of time to process will be split into\n    many intervals (currently only support days) given that the range spans\n    multiple intervals.\n\n    If the range does not span multiple intervals, then the result is one\n    interval, depending on the value of the include_now parameter.\n\n    Unless include_now is True, the last interval in the list\n    will be the last full interval in the range.  For example, since we only\n    support days, if the last offset is today, there should be no intervals\n    to process, becuase the default is up through yesterday (UTC). If include_now\n    is True, then that final interval in the list will end before now (UTC) or the\n    end_date_override.\n    The existence of end_date_override ignores include_now = False.\n\n    Args:\n        last_offset (str): Last offset to use for getting proper intervals.\n        If None, then defaults to value of override or initial offsets.\n\n    Returns:\n        list[IntervalSet]: List of intervals to use for job processing.\n    \"\"\"\n    # use override end date if provided\n    # default to now UTC\n    batch_end = self.override_batch_end or pdm.now(tz=\"UTC\").to_iso8601_string()\n    # if last_offset is None, we use the initial offset\n    if last_offset is None or last_offset == \"None\":\n        last_offset = self.initial_last_offset\n    # override last offset if provided\n    last_offset_str = self.override_last_offset or str(last_offset)\n    # the resulting last_offset_str cannot be None\n    if last_offset_str is None or last_offset_str == \"None\":\n        raise ValueError(\n            \"The resulting last offset cannot be None. \"\n            \"If last_offset is None, then initial_last_offset or \"\n            \"override_last_offset must be set\"\n        )\n    # offset will be incremented by 1 microseconds\n    # to support using '&gt;=' and '&lt;' for all intervals\n    # this is less aggressive than using 1000 (1 millisecond)\n    last_offset_final = parse(last_offset_str).add(microseconds=1)  # type: ignore\n    # the calculated intervals should start from\n    # the first full interval after the last offset\n    start = last_offset_final.end_of(\"day\").add(microseconds=1)  # type: ignore\n    end = parse(batch_end)\n    # create a pendulum period\n    period_range = pdm.period(start, end, absolute=True)  # type: ignore\n    # create base list of datetime object from period\n    # only include if less then of equal to start of period\n    # end datetime\n    intervals = [\n        x\n        for x in period_range.range(\"days\")\n        if x &lt;= end.start_of(\"day\")  # type: ignore\n    ]\n    if intervals:\n        # if we have intervals, insert offset as the first\n        intervals.insert(0, last_offset_final)\n    # setting base end to the last item in list\n    # this is for include_now and override logic\n    base_end = intervals[-1]\n    # only append base_end if needed and &gt; then base_end\n    if self.include_now or self.override_batch_end:\n        if end &gt; base_end:  # type: ignore\n            intervals.append(end)\n            base_end = end\n    # create base parameters for the for loop\n    interval_len = len(intervals)\n    last_idx = interval_len - 2\n    # create the staging list of intervat sets\n    interval_sets = []\n    # set proper first_interval_start\n    first_interval_start = start\n    first_item_time = intervals[0].to_time_string()  # type: ignore\n    # this evaluation is needed because if the offset value...\n    # turns out to be the start of the day, then that is really...\n    # the first interval start and helper with more efficient file clean up\n    if first_item_time == \"00:00:00\":\n        first_interval_start = intervals[0]\n    for idx, i in enumerate(intervals):\n        is_initial = False\n        is_final = False\n        if idx == 0:\n            is_initial = True\n        if idx == last_idx:\n            is_final = True\n        end_idx = idx + 1\n        # last item in list should not be used as start\n        if end_idx == interval_len:\n            break\n        interval_sets.append(\n            IntervalSet(\n                batch_start=i.to_iso8601_string(),  # type: ignore\n                batch_end=intervals[end_idx].to_iso8601_string(),  # type: ignore\n                first_interval_start=first_interval_start.to_iso8601_string(),  # type: ignore  # noqa: E501\n                sets_end=base_end.to_iso8601_string(),  # type: ignore\n                is_initial=is_initial,\n                is_final=is_final,\n            )\n        )\n\n    return interval_sets\n</code></pre>"},{"location":"shared/utils/code/#src.shared.utils.SqlJob.render_from_template","title":"<code>render_from_template</code>","text":"<p>Helper function</p> <p>Parameters:</p> Name Type Description Default <code>template</code> <code>Template</code> <p>jinja2 Template object to render.</p> required <code>render_kwargs</code> <code>dict</code> <p>kwargs to pass to render function.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>Exception that enforces existence of sql engine variable.</p> <p>Returns:</p> Name Type Description <code>SqlStmt</code> <code>SqlStmt</code> <p>SqlStmt object with sql text and db engine.</p> Source code in <code>src/shared/utils.py</code> <pre><code>def render_from_template(self, template: Template, render_kwargs: dict) -&gt; SqlStmt:\n\"\"\"Helper function\n\n    Args:\n        template (Template): jinja2 Template object to render.\n        render_kwargs (dict): kwargs to pass to render function.\n\n    Raises:\n        Exception: Exception that enforces existence of sql engine variable.\n\n    Returns:\n        SqlStmt: SqlStmt object with sql text and db engine.\n    \"\"\"\n\n    sql_engine = \"snowflake\"\n    is_multi_statement = False\n    try:\n        sql_engine = template.module.sql_engine  # type: ignore\n    except AttributeError:\n        raise Exception(\n            'SQL file must have jinja2 block {% set sql_engine = \"&lt;engine_type&gt;\" %}'\n            f\", and must be one of {QUERY_ENGINE_TYPES_SET}\"\n        )\n    try:\n        is_multi_statement = template.module.is_multi_statement  # type: ignore\n    except AttributeError:\n        pass\n    sql_text = template.render(**render_kwargs)\n    return SqlStmt(\n        sql_engine=sql_engine,\n        sql_text=sql_text,\n        is_multi_statement=is_multi_statement,\n        connection_overrides=self.connection_overrides,\n    )\n</code></pre>"},{"location":"shared/utils/code/#src.shared.utils.SqlJob.render_sql_file","title":"<code>render_sql_file</code>","text":"<p>Helper method for rendering a jinja2 sql file using job kwargs plus optional additional kwargs.</p> <p>Parameters:</p> Name Type Description Default <code>sql_file</code> <code>str</code> <p>File name containing SQL text with jijna2 template logic.</p> required <code>extra_kwargs</code> <code>dict</code> <p>Optional kwargs to pass into template on top of</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>SqlStmt</code> <code>SqlStmt</code> <p>SqlStmt object with sql text and db engine.</p> Source code in <code>src/shared/utils.py</code> <pre><code>def render_sql_file(self, sql_file: str, extra_kwargs: dict = {}) -&gt; SqlStmt:\n\"\"\"Helper method for rendering a jinja2 sql file using\n    job kwargs plus optional additional kwargs.\n\n    Args:\n        sql_file (str): File name containing SQL text with jijna2 template logic.\n        extra_kwargs (dict): Optional kwargs to pass into template on top of\n        the job_kwargs.\n\n    Returns:\n        SqlStmt: SqlStmt object with sql text and db engine.\n    \"\"\"\n    render_kwargs = deepcopy(self.job_kwargs)\n    render_kwargs.update(extra_kwargs)\n    environment = Environment(\n        loader=FileSystemLoader([self.job_file_path, self.extras_file_path]),\n    )\n    j2_env = environment\n    template = j2_env.get_template(sql_file)\n    return self.render_from_template(template, render_kwargs)\n</code></pre>"},{"location":"shared/utils/code/#src.shared.utils.SqlJob.render_sql_string","title":"<code>render_sql_string</code>","text":"<p>Helper method for rendering a jinj2 sql string using job kwargs plus optional additional kwargs.</p> <p>Parameters:</p> Name Type Description Default <code>sql_string</code> <code>str</code> <p>SQL text with jijna2 template logic.</p> required <code>extra_kwargs</code> <code>dict</code> <p>Optional kwargs to pass into template on top of</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>SqlStmt</code> <code>SqlStmt</code> <p>SqlStmt object with sql text and db engine.</p> Source code in <code>src/shared/utils.py</code> <pre><code>def render_sql_string(self, sql_string: str, extra_kwargs: dict = {}) -&gt; SqlStmt:\n\"\"\"Helper method for rendering a jinj2 sql string\n    using job kwargs plus optional additional kwargs.\n\n    Args:\n        sql_string (str): SQL text with jijna2 template logic.\n        extra_kwargs (dict): Optional kwargs to pass into template on top of\n        the job_kwargs.\n\n    Returns:\n        SqlStmt: SqlStmt object with sql text and db engine.\n    \"\"\"\n    render_kwargs = deepcopy(self.job_kwargs)\n    render_kwargs.update(extra_kwargs)\n    environment = Environment()\n    j2_env = environment\n    template = j2_env.from_string(sql_string)\n    return self.render_from_template(template, render_kwargs)\n</code></pre>"},{"location":"shared/utils/code/#src.shared.utils.SqlStmt","title":"<code>SqlStmt</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Pydantic model representing a sql statement with db engine to run on.</p> Source code in <code>src/shared/utils.py</code> <pre><code>class SqlStmt(BaseModel):\n\"\"\"Pydantic model representing a sql statement with\n    db engine to run on.\n    \"\"\"\n\n    sql_text: str\n    sql_engine: QUERY_ENGINE_TYPES_LITERAL\n    is_multi_statement: bool\n    connection_overrides: dict\n    _creds_param_name: str = PrivateAttr()\n    _creds_param_value: Any = PrivateAttr()\n\n    def _init_private_attributes(self) -&gt; None:\n\"\"\"Great way to update private attributes without\n        losing autocomplete on the model.\n\n        https://github.com/pydantic/pydantic/discussions/3512#discussioncomment-3226167\n\n        \"\"\"\n        super()._init_private_attributes()\n        for k, v in QUERY_ENGINE_MAPPING[self.sql_engine].items():\n            self._creds_param_name = k\n            self._creds_param_value = v\n\n    @property\n    def standard_kwargs(self) -&gt; dict:\n\"\"\"Returns a standard set of kwargs for query task\n        based on engine that gets passed to run_query_task.\n\n        Returns:\n            dict: standard kwargs.\n        \"\"\"\n        return {\n            \"query\": self.sql_text,\n            self._creds_param_name: self._creds_param_value(\n                **self.connection_overrides.get(self._creds_param_name, {})\n            ),\n        }\n\n    async def run_query_task(self, task_name: str, **kwargs) -&gt; list[tuple]:\n\"\"\"Helper method to dynamically execute the proper\n        query task based on sql statement attributes.\n\n        Args:\n            task_name (str): Name to pass to query task\n\n        Returns:\n            list[tuple]: Query results.\n        \"\"\"\n        task_mapping = {\n            \"snowflake\": {\"single\": snowflake_query, \"multi\": snowflake_multiquery},\n            \"bigquery\": {\"single\": bigquery_query},\n            \"default\": {\"single\": sqlalchemy_execute},\n        }\n        standard_kwargs = self.standard_kwargs\n        query_task = task_mapping[\"default\"][\"single\"]\n        sql_engine = self.sql_engine\n        is_multi_statement = self.is_multi_statement\n        if sql_engine in [\"snowflake\", \"bigquery\"]:\n            if sql_engine == \"snowflake\" and is_multi_statement:\n                query_task = task_mapping[sql_engine][\"multi\"]\n                standard_kwargs[\"queries\"] = (\n                    standard_kwargs[\"query\"].rstrip().rstrip(\";\").split(\";\")\n                )\n                standard_kwargs.pop(\"query\")\n            else:\n                query_task = task_mapping[sql_engine][\"single\"]\n        else:\n            standard_kwargs[\"statement\"] = standard_kwargs[\"query\"]\n            standard_kwargs.pop(\"query\")\n        kwargs.update(standard_kwargs)\n        return await query_task.with_options(name=task_name)(**kwargs)\n</code></pre>"},{"location":"shared/utils/code/#src.shared.utils.SqlStmt-attributes","title":"Attributes","text":""},{"location":"shared/utils/code/#src.shared.utils.SqlStmt.standard_kwargs","title":"<code>standard_kwargs: dict</code>  <code>property</code>","text":"<p>Returns a standard set of kwargs for query task based on engine that gets passed to run_query_task.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>standard kwargs.</p>"},{"location":"shared/utils/code/#src.shared.utils.SqlStmt-functions","title":"Functions","text":""},{"location":"shared/utils/code/#src.shared.utils.SqlStmt.run_query_task","title":"<code>run_query_task</code>  <code>async</code>","text":"<p>Helper method to dynamically execute the proper query task based on sql statement attributes.</p> <p>Parameters:</p> Name Type Description Default <code>task_name</code> <code>str</code> <p>Name to pass to query task</p> required <p>Returns:</p> Type Description <code>list[tuple]</code> <p>list[tuple]: Query results.</p> Source code in <code>src/shared/utils.py</code> <pre><code>async def run_query_task(self, task_name: str, **kwargs) -&gt; list[tuple]:\n\"\"\"Helper method to dynamically execute the proper\n    query task based on sql statement attributes.\n\n    Args:\n        task_name (str): Name to pass to query task\n\n    Returns:\n        list[tuple]: Query results.\n    \"\"\"\n    task_mapping = {\n        \"snowflake\": {\"single\": snowflake_query, \"multi\": snowflake_multiquery},\n        \"bigquery\": {\"single\": bigquery_query},\n        \"default\": {\"single\": sqlalchemy_execute},\n    }\n    standard_kwargs = self.standard_kwargs\n    query_task = task_mapping[\"default\"][\"single\"]\n    sql_engine = self.sql_engine\n    is_multi_statement = self.is_multi_statement\n    if sql_engine in [\"snowflake\", \"bigquery\"]:\n        if sql_engine == \"snowflake\" and is_multi_statement:\n            query_task = task_mapping[sql_engine][\"multi\"]\n            standard_kwargs[\"queries\"] = (\n                standard_kwargs[\"query\"].rstrip().rstrip(\";\").split(\";\")\n            )\n            standard_kwargs.pop(\"query\")\n        else:\n            query_task = task_mapping[sql_engine][\"single\"]\n    else:\n        standard_kwargs[\"statement\"] = standard_kwargs[\"query\"]\n        standard_kwargs.pop(\"query\")\n    kwargs.update(standard_kwargs)\n    return await query_task.with_options(name=task_name)(**kwargs)\n</code></pre>"},{"location":"shared/utils/code/#src.shared.utils-functions","title":"Functions","text":""},{"location":"shared/utils/code/#src.shared.utils.get_files_for_cleanup","title":"<code>get_files_for_cleanup</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>file_list</code> <code>list[tuple]</code> <p>File list pulled from warehouse stage.</p> required <code>interval_input</code> <code>IntervalSet</code> <p>Interval metadata.</p> required <code>block_storage_prefix</code> <code>str</code> <p>Block storage prefex used for files.</p> <code>'gcs'</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of staged file paths for deletion.</p> Source code in <code>src/shared/utils.py</code> <pre><code>@task()\ndef get_files_for_cleanup(\n    file_list: list[tuple],\n    interval_input: IntervalSet,\n    block_storage_prefix: str = \"gcs\",\n) -&gt; list[str]:\n\"\"\"_summary_\n\n    Args:\n        file_list (list[tuple]): File list pulled from warehouse stage.\n        interval_input (IntervalSet): Interval metadata.\n        block_storage_prefix (str, optional): Block storage prefex used for files.\n        This helps for deconstructing the path. Defaults to \"gcs\".\n\n    Returns:\n        list[str]: List of staged file paths for deletion.\n    \"\"\"\n    # get the timestamp used for the partition folders\n    batch_folder_datetime = interval_input.partition_datetime\n    # get datetime object for first full day interval start\n    # we will delete all file paths after the base start\n    date_folder_base_start = parse(interval_input.first_interval_start)  # type: ignore\n    date_folder_base_end = parse(interval_input.sets_end)  # type: ignore\n    # initial list for collecting file paths\n    clean_up_list = []\n    # loop through the input file and append file paths to final results...\n    # as needed.\n    for f in file_list:\n        file_str = f[0].replace(f\"{block_storage_prefix}://\", \"\")\n        file_parts = Path(file_str).parts\n        date_folder_str = file_parts[-3]\n        datetime_folder_str = file_parts[-3] + \"/\" + file_parts[-2]\n        datetime_str = (\n            datetime_folder_str.replace(\"date=\", \"\")\n            .replace(\"time=\", \"\")\n            .replace(\"/\", \" \")\n        )\n        # the goal of above is to infer datetime from the existing file paths\n        parsed_datetime = from_format(\n            datetime_str, f\"YYYY-MM-DD {interval_input.time_format_string}\"\n        )  # noqa: E501\n        # only delete files after interval start and before last interval end\n        if parsed_datetime &gt;= batch_folder_datetime:  # type: ignore\n            if parsed_datetime &lt; date_folder_base_end:  # type: ignore\n                # if the datetime from folder is greater than the first full day start...  # noqa: E501\n                # add the date folder and break loop as there is no need to check any more  # noqa: E501\n                if parsed_datetime &gt;= date_folder_base_start:  # type: ignore\n                    clean_up_list.append(date_folder_str)\n                    break\n                clean_up_list.append(datetime_folder_str)\n    return list(set(clean_up_list))\n</code></pre>"}]}